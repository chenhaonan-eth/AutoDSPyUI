# æ¨¡å‹éƒ¨ç½²ä¸æ•°æ®é£è½®æ–¹æ¡ˆ

æœ¬æ–‡æ¡£æè¿° DSPyUI çš„æ¨¡å‹éƒ¨ç½²ã€æ¨ç†æœåŠ¡ã€ç”¨æˆ·åé¦ˆæ”¶é›†å’Œæ•°æ®é£è½®é—­ç¯æ–¹æ¡ˆã€‚

## å¿«é€Ÿå¼€å§‹

### å¯åŠ¨ API æœåŠ¡

```bash
# æ–¹å¼ä¸€ï¼šä»…å¯åŠ¨ API æœåŠ¡ï¼ˆæ¨èç”¨äºç”Ÿäº§ï¼‰
bash webui.sh --api-only

# æ–¹å¼äºŒï¼šåŒæ—¶å¯åŠ¨ Gradio UI å’Œ API æœåŠ¡
bash webui.sh --api

# æ–¹å¼ä¸‰ï¼šç›´æ¥ä½¿ç”¨ Python å¯åŠ¨
uv run python serve.py

# æ–¹å¼å››ï¼šä½¿ç”¨ uvicorn å¯åŠ¨ï¼ˆæ”¯æŒæ›´å¤šé…ç½®ï¼‰
uv run uvicorn dspyui.api.app:app --host 0.0.0.0 --port 8000 --workers 4
```

### éªŒè¯æœåŠ¡çŠ¶æ€

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æŸ¥çœ‹ API æ–‡æ¡£
open http://localhost:8000/docs
```

## æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ•°æ®é£è½®é—­ç¯                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚  è®­ç»ƒ    â”‚â”€â”€â”€â–¶â”‚  æ³¨å†Œ    â”‚â”€â”€â”€â–¶â”‚  éƒ¨ç½²    â”‚â”€â”€â”€â–¶â”‚  æ¨ç†    â”‚ â”‚
â”‚   â”‚ Compile  â”‚    â”‚ Registry â”‚    â”‚  Serve   â”‚    â”‚ Predict  â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        â–²                                               â”‚       â”‚
â”‚        â”‚                                               â–¼       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚ æ•°æ®å¯¼å‡º â”‚â—€â”€â”€â”€â”‚  è¯„ä¼°    â”‚â—€â”€â”€â”€â”‚  å­˜å‚¨    â”‚â—€â”€â”€â”€â”‚  åé¦ˆ    â”‚ â”‚
â”‚   â”‚  Export  â”‚    â”‚ Evaluate â”‚    â”‚ MLflow   â”‚    â”‚ ğŸ‘ğŸ‘ğŸ’¬   â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## MLflow å…¨èƒ½åŠ›åˆ©ç”¨

| MLflow èƒ½åŠ› | åº”ç”¨åœºæ™¯ | API |
|------------|---------|-----|
| Dataset Tracking | è®­ç»ƒæ•°æ®ç‰ˆæœ¬ç®¡ç† | `mlflow.log_input()`, `mlflow.data.from_pandas()` |
| Experiment Tracking | ç¼–è¯‘å‚æ•°/æŒ‡æ ‡è®°å½• | `mlflow.log_param()`, `mlflow.log_metric()` |
| Model Registry | æ¨¡å‹ç‰ˆæœ¬ç®¡ç† | `mlflow.register_model()` |
| Model Serving | éƒ¨ç½²æ¨ç†æœåŠ¡ | `mlflow models serve` |
| Tracing | æ¨ç†è¯·æ±‚è¿½è¸ª | `mlflow.start_span()`, è‡ªåŠ¨è¿½è¸ª |
| Feedback/Assessment | ç”¨æˆ·åé¦ˆæ”¶é›† | `mlflow.log_feedback()` |
| GenAI Evaluate | åŸºäºåé¦ˆè¯„ä¼° | `mlflow.genai.evaluate()` |

---

## 1. æ¨¡å‹éƒ¨ç½²

### 1.1 éƒ¨ç½²æ–¹å¼

#### æ–¹å¼ä¸€ï¼šFastAPI éƒ¨ç½²ï¼ˆæ¨èï¼‰

DSPyUI å†…ç½®äº†å®Œæ•´çš„ FastAPI æœåŠ¡ï¼Œæ”¯æŒå¼‚æ­¥é«˜å¹¶å‘ï¼š

```bash
# å¯åŠ¨ API æœåŠ¡
uv run python serve.py

# æˆ–ä½¿ç”¨ webui.sh
bash webui.sh --api-only
```

æœåŠ¡å¯åŠ¨åå¯è®¿é—®ï¼š
- API ç«¯ç‚¹: http://localhost:8000
- Swagger æ–‡æ¡£: http://localhost:8000/docs
- ReDoc æ–‡æ¡£: http://localhost:8000/redoc

#### æ–¹å¼äºŒï¼šMLflow Model Serving

æ ‡å‡†åŒ–éƒ¨ç½²ï¼Œæ”¯æŒ Docker/Kubernetesï¼š

```bash
# æœ¬åœ°éƒ¨ç½²
mlflow models serve -m "models:/joke-generator@Production" -p 6000

# æ„å»º Docker é•œåƒ
mlflow models build-docker -m "models:/joke-generator/3" -n "dspy-program"

# è°ƒç”¨
curl -X POST http://localhost:6000/invocations \
  -H "Content-Type: application/json" \
  -d '{"inputs": {"topic": "çŒ«"}}'
```

### 1.2 API ç«¯ç‚¹ä¸€è§ˆ

| ç«¯ç‚¹ | æ–¹æ³• | æè¿° |
|------|------|------|
| `/predict` | POST | æ‰§è¡Œæ¨¡å‹æ¨ç† |
| `/feedback` | POST | æäº¤ç”¨æˆ·åé¦ˆ |
| `/export` | GET | å¯¼å‡ºè®­ç»ƒæ•°æ® |
| `/models` | GET | åˆ—å‡ºæ‰€æœ‰æ¨¡å‹ |
| `/models/{name}/versions` | GET | åˆ—å‡ºæ¨¡å‹ç‰ˆæœ¬ |
| `/models/{name}/stage` | POST | åˆ‡æ¢æ¨¡å‹é˜¶æ®µ |
| `/health` | GET | å¥åº·æ£€æŸ¥ |
| `/metrics` | GET | æœåŠ¡æŒ‡æ ‡ |

---

## 2. æ¨¡å‹ç‰ˆæœ¬åˆ‡æ¢

### 2.1 åˆ‡æ¢ç­–ç•¥

#### åŸºäºç‰ˆæœ¬å·ï¼ˆç²¾ç¡®æ§åˆ¶ï¼‰

```python
model_uri = "models:/joke-generator/3"  # æŒ‡å®šç‰ˆæœ¬ 3
```

#### åŸºäºé˜¶æ®µï¼ˆæ¨èç”Ÿäº§ä½¿ç”¨ï¼‰

```python
model_uri = "models:/joke-generator@Production"  # ç”Ÿäº§ç‰ˆæœ¬
model_uri = "models:/joke-generator@Staging"     # æµ‹è¯•ç‰ˆæœ¬
```

#### åŸºäºåˆ«åï¼ˆMLflow 2.xï¼‰

```python
# è®¾ç½®åˆ«å
client.set_registered_model_alias("joke-generator", "champion", "3")

# ä½¿ç”¨åˆ«å
model_uri = "models:/joke-generator@champion"
```

### 2.2 é˜¶æ®µæµè½¬

```
None â†’ Staging â†’ Production â†’ Archived
       (æµ‹è¯•)     (ç”Ÿäº§)       (å½’æ¡£)
```

åˆ‡æ¢æ–¹å¼ï¼š
- **MLflow UI**ï¼šModels â†’ é€‰æ‹©ç‰ˆæœ¬ â†’ Stage ä¸‹æ‹‰æ¡†
- **API è°ƒç”¨**ï¼š`POST /models/{name}/stage`
- **CLI**ï¼š`mlflow models transition-stage`

### 2.3 åˆ‡æ¢æµç¨‹ç¤ºä¾‹

```
T1: Version 3 @ Production (çº¿ä¸ŠæœåŠ¡)
    â†“
T2: è®­ç»ƒå®Œæˆ Version 4ï¼Œæ³¨å†Œä¸º None
    â†“
T3: æµ‹è¯•é€šè¿‡ï¼ŒVersion 4 â†’ Staging
    â†“
T4: éªŒè¯ OKï¼ŒVersion 4 â†’ Production (Version 3 è‡ªåŠ¨å½’æ¡£)
    â†“
T5: ä¸‹ä¸€æ¬¡è¯·æ±‚è‡ªåŠ¨ä½¿ç”¨ Version 4 (æ— éœ€é‡å¯ï¼)
```

---

## 3. ç”¨æˆ·åé¦ˆæ”¶é›†

### 3.1 å­˜å‚¨ä½ç½®

åé¦ˆç›´æ¥å­˜å…¥ **MLflow Tracking Store**ï¼Œä¸ Trace å…³è”ï¼š

```
mlruns/
â”œâ”€â”€ experiments/{experiment_id}/{run_id}/
â”‚   â””â”€â”€ traces/{trace_id}/
â”‚       â”œâ”€â”€ spans/           # è°ƒç”¨é“¾
â”‚       â””â”€â”€ assessments/     # åé¦ˆæ•°æ® âœ¨
â”‚           â”œâ”€â”€ user_rating
â”‚           â”œâ”€â”€ corrected_output
â”‚           â””â”€â”€ comment
```

### 3.2 åé¦ˆ API

```
POST /feedback
  è¯·æ±‚: {
    "trace_id": "xxx",
    "rating": "thumbs_up",           # thumbs_up / thumbs_down
    "corrected_output": "...",       # å¯é€‰ï¼šç”¨æˆ·ä¿®æ­£
    "comment": "å¾ˆæœ‰è¶£"              # å¯é€‰ï¼šè¯„è®º
  }
```

### 3.3 åé¦ˆç±»å‹

| ç±»å‹ | å­—æ®µ | è¯´æ˜ |
|------|------|------|
| è¯„åˆ† | `user_rating` | thumbs_up / thumbs_down |
| ä¿®æ­£ | `corrected_output` | ç”¨æˆ·æä¾›çš„æ­£ç¡®è¾“å‡º |
| è¯„è®º | `comment` | æ–‡å­—åé¦ˆ |

### 3.4 ä»£ç å®ç°

```python
import mlflow
from mlflow.entities import AssessmentSource, AssessmentSourceType

@app.post("/feedback")
async def submit_feedback(request: FeedbackRequest):
    # è®°å½•ç”¨æˆ·è¯„åˆ†
    mlflow.log_feedback(
        trace_id=request.trace_id,
        name="user_rating",
        value=request.rating,
        source=AssessmentSource(
            source_type=AssessmentSourceType.HUMAN,
            source_id=request.user_id or "anonymous"
        )
    )
    
    # è®°å½•ç”¨æˆ·ä¿®æ­£ï¼ˆå¦‚æœæœ‰ï¼‰
    if request.corrected_output:
        mlflow.log_feedback(
            trace_id=request.trace_id,
            name="corrected_output",
            value=request.corrected_output,
            source=AssessmentSource(source_type=AssessmentSourceType.HUMAN)
        )
    
    # è®°å½•è¯„è®ºï¼ˆå¦‚æœæœ‰ï¼‰
    if request.comment:
        mlflow.log_feedback(
            trace_id=request.trace_id,
            name="comment",
            value=request.comment,
            source=AssessmentSource(source_type=AssessmentSourceType.HUMAN)
        )
    
    return {"status": "success"}
```

---

## 4. æ•°æ®å¯¼å‡ºä¸é£è½®é—­ç¯

### 4.1 å¯¼å‡ºé«˜è´¨é‡æ•°æ®

```python
@app.get("/export")
async def export_training_data(
    model_name: str,
    rating: str = "thumbs_up",
    format: str = "csv"
):
    # æŸ¥è¯¢å¸¦æ­£å‘åé¦ˆçš„ traces
    traces_df = mlflow.search_traces(
        filter_string=f"assessments.name = 'user_rating' AND assessments.value = '{rating}'"
    )
    
    # è½¬æ¢ä¸ºè®­ç»ƒæ•°æ®æ ¼å¼
    training_data = []
    for _, trace in traces_df.iterrows():
        # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·ä¿®æ­£ï¼Œå¦åˆ™ç”¨åŸå§‹è¾“å‡º
        corrected = get_assessment_value(trace, "corrected_output")
        training_data.append({
            **trace.inputs,
            **{k: corrected.get(k) or v for k, v in trace.outputs.items()}
        })
    
    df = pd.DataFrame(training_data)
    
    if format == "csv":
        return Response(df.to_csv(index=False), media_type="text/csv")
    else:
        return df.to_dict(orient="records")
```

### 4.2 æ•°æ®é£è½®æµç¨‹

```
1. éƒ¨ç½²æ¨¡å‹ â†’ ç”¨æˆ·ä½¿ç”¨
2. æ”¶é›†åé¦ˆ â†’ å­˜å…¥ MLflow
3. å¯¼å‡ºæ•°æ® â†’ è¿‡æ»¤é«˜è´¨é‡æ ·æœ¬
4. é‡æ–°è®­ç»ƒ â†’ ä½¿ç”¨ DSPyUI Compile
5. æ³¨å†Œæ–°ç‰ˆ â†’ æå‡åˆ° Production
6. å¾ªç¯è¿­ä»£
```

### 4.3 å¯¼å‡º API

```
GET /export?model=joke-generator&rating=thumbs_up&format=csv

å“åº”: CSV æ–‡ä»¶ï¼ŒåŒ…å«è¾“å…¥è¾“å‡ºå­—æ®µ
```

---

## 5. æ–‡ä»¶ç»“æ„

```
dspyui/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ serving.py       # æ¨ç†æœåŠ¡æ ¸å¿ƒé€»è¾‘
â”‚   â”œâ”€â”€ feedback.py      # åé¦ˆæ”¶é›†ï¼ˆå°è£… mlflow.log_feedbackï¼‰
â”‚   â””â”€â”€ model_manager.py # æ¨¡å‹åŠ è½½å’Œç‰ˆæœ¬ç®¡ç†
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py           # FastAPI åº”ç”¨
â”‚   â”œâ”€â”€ schemas.py       # Pydantic æ¨¡å‹
â”‚   â””â”€â”€ routes/
â”‚       â”œâ”€â”€ predict.py   # æ¨ç†è·¯ç”±
â”‚       â”œâ”€â”€ models.py    # æ¨¡å‹ç®¡ç†è·¯ç”±
â”‚       â”œâ”€â”€ feedback.py  # åé¦ˆè·¯ç”±
â”‚       â””â”€â”€ export.py    # æ•°æ®å¯¼å‡ºè·¯ç”±
serve.py                 # API æœåŠ¡å…¥å£
```

---

## 6. ç¯å¢ƒå˜é‡

åœ¨ `.env` ä¸­æ·»åŠ ï¼š

```bash
# API æœåŠ¡é…ç½®
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# æ¨¡å‹é»˜è®¤é…ç½®
DEFAULT_MODEL_STAGE=Production
MODEL_CACHE_ENABLED=true

# åé¦ˆé…ç½®
FEEDBACK_ENABLED=true
```

---

## 7. å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨ API æœåŠ¡
uv run python serve.py

# æˆ–ä½¿ç”¨ uvicorn
uv run uvicorn dspyui.api.app:app --host 0.0.0.0 --port 8000 --workers 4
```

---

## 8. ä½¿ç”¨ç¤ºä¾‹

### 8.1 å®Œæ•´æ•°æ®é£è½®æµç¨‹

```bash
# 1. å¯åŠ¨æœåŠ¡ï¼ˆåŒ…å« MLflowï¼‰
bash webui.sh --api-only

# 2. æ‰§è¡Œæ¨ç†
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "model": "joke-generator",
    "inputs": {"topic": "çŒ«"},
    "stage": "Production"
  }'

# å“åº”ç¤ºä¾‹:
# {
#   "result": {"joke": "ä¸ºä»€ä¹ˆçŒ«ä¸ç©æ‰‘å…‹ï¼Ÿå› ä¸ºå®ƒæ€»æ˜¯åœ¨æ¡Œå­ä¸Šç¡è§‰ï¼"},
#   "trace_id": "tr-abc123def456",
#   "model_version": "3",
#   "latency_ms": 1234.56
# }

# 3. æäº¤ç”¨æˆ·åé¦ˆï¼ˆæ­£å‘ï¼‰
curl -X POST http://localhost:8000/feedback \
  -H "Content-Type: application/json" \
  -d '{
    "trace_id": "tr-abc123def456",
    "rating": "thumbs_up",
    "comment": "å¾ˆæœ‰è¶£çš„ç¬‘è¯ï¼"
  }'

# 4. æäº¤ç”¨æˆ·åé¦ˆï¼ˆè´Ÿå‘ + ä¿®æ­£ï¼‰
curl -X POST http://localhost:8000/feedback \
  -H "Content-Type: application/json" \
  -d '{
    "trace_id": "tr-xyz789",
    "rating": "thumbs_down",
    "corrected_output": {"joke": "æ›´å¥½çš„ç¬‘è¯å†…å®¹..."},
    "comment": "åŸæ¥çš„ä¸å¤Ÿæœ‰è¶£"
  }'

# 5. å¯¼å‡ºé«˜è´¨é‡è®­ç»ƒæ•°æ®
curl "http://localhost:8000/export?model=joke-generator&rating=thumbs_up&format=csv" \
  -o training_data.csv

# 6. æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨
curl http://localhost:8000/models

# 7. æŸ¥çœ‹æ¨¡å‹ç‰ˆæœ¬
curl http://localhost:8000/models/joke-generator/versions

# 8. åˆ‡æ¢æ¨¡å‹åˆ° Production
curl -X POST http://localhost:8000/models/joke-generator/stage \
  -H "Content-Type: application/json" \
  -d '{"version": "4", "stage": "Production"}'

# 9. å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# å“åº”ç¤ºä¾‹:
# {
#   "status": "healthy",
#   "mlflow_connected": true,
#   "loaded_models_count": 2,
#   "uptime_seconds": 3600.5
# }

# 10. æŸ¥çœ‹æœåŠ¡æŒ‡æ ‡
curl http://localhost:8000/metrics

# å“åº”ç¤ºä¾‹:
# {
#   "request_count": 150,
#   "error_count": 3,
#   "average_latency_ms": 1456.78,
#   "models_served": ["joke-generator", "text-rewriter"]
# }
```

### 8.2 Python SDK ä½¿ç”¨ç¤ºä¾‹

```python
import requests

API_BASE = "http://localhost:8000"

# æ¨ç†
def predict(model: str, inputs: dict, stage: str = "Production"):
    response = requests.post(
        f"{API_BASE}/predict",
        json={"model": model, "inputs": inputs, "stage": stage}
    )
    return response.json()

# æäº¤åé¦ˆ
def submit_feedback(trace_id: str, rating: str, corrected_output: dict = None, comment: str = None):
    payload = {"trace_id": trace_id, "rating": rating}
    if corrected_output:
        payload["corrected_output"] = corrected_output
    if comment:
        payload["comment"] = comment
    
    response = requests.post(f"{API_BASE}/feedback", json=payload)
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
result = predict("joke-generator", {"topic": "ç¨‹åºå‘˜"})
print(f"ç¬‘è¯: {result['result']['joke']}")
print(f"Trace ID: {result['trace_id']}")

# ç”¨æˆ·è§‰å¾—å¥½ç¬‘ï¼Œæäº¤æ­£å‘åé¦ˆ
submit_feedback(result['trace_id'], "thumbs_up", comment="å“ˆå“ˆå¤ªçœŸå®äº†")
```

### 8.3 å¼‚æ­¥æ‰¹é‡æ¨ç†

```python
import asyncio
import aiohttp

async def batch_predict(inputs_list: list, model: str = "joke-generator"):
    """æ‰¹é‡å¼‚æ­¥æ¨ç†"""
    async with aiohttp.ClientSession() as session:
        tasks = []
        for inputs in inputs_list:
            task = session.post(
                "http://localhost:8000/predict",
                json={"model": model, "inputs": inputs}
            )
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks)
        results = [await r.json() for r in responses]
        return results

# ä½¿ç”¨ç¤ºä¾‹
topics = [{"topic": "çŒ«"}, {"topic": "ç‹—"}, {"topic": "ç¨‹åºå‘˜"}]
results = asyncio.run(batch_predict(topics))
for r in results:
    print(f"Topic: {r['result']}, Trace: {r['trace_id']}")
```

---

## 9. ä¼˜åŠ¿æ€»ç»“

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| é›¶é¢å¤–å­˜å‚¨ | åé¦ˆç›´æ¥å­˜ MLflowï¼Œæ— éœ€é¢å¤–æ•°æ®åº“ |
| å®Œæ•´è¡€ç¼˜ | trace_id å…³è” è¾“å…¥â†’è¾“å‡ºâ†’åé¦ˆ |
| åŸç”ŸæŸ¥è¯¢ | ç”¨ MLflow API è¿‡æ»¤é«˜è´¨é‡æ•°æ® |
| UI å¯è§†åŒ– | MLflow UI ç›´æ¥æŸ¥çœ‹åé¦ˆ |
| åŠ¨æ€åˆ‡æ¢ | æ¨¡å‹ç‰ˆæœ¬å³æ—¶åˆ‡æ¢ï¼Œæ— éœ€é‡å¯ |
| é—­ç¯è¿­ä»£ | æ•°æ®é£è½®è‡ªåŠ¨åŒ– |
| å¼‚æ­¥é«˜å¹¶å‘ | æ”¯æŒ asyncify å’Œå¤š worker |
| è¶…æ—¶ä¿æŠ¤ | å¯é…ç½®è¯·æ±‚è¶…æ—¶ï¼Œé˜²æ­¢é˜»å¡ |
| ä¼˜é›…é™çº§ | MLflow ä¸å¯ç”¨æ—¶ä»å¯æœåŠ¡ |

---

## 10. æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

**Q: API æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Ÿ**
```bash
# æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
lsof -i :8000

# æ£€æŸ¥ MLflow è¿æ¥
curl http://localhost:5001/health
```

**Q: æ¨ç†è¿”å› 404 Model not foundï¼Ÿ**
```bash
# ç¡®è®¤æ¨¡å‹å·²æ³¨å†Œ
curl http://localhost:8000/models

# æ£€æŸ¥æ¨¡å‹é˜¶æ®µ
curl http://localhost:8000/models/your-model/versions
```

**Q: åé¦ˆæäº¤è¿”å› 404 Trace not foundï¼Ÿ**
- ç¡®ä¿ trace_id æ¥è‡ªæœ€è¿‘çš„æ¨ç†å“åº”
- æ£€æŸ¥ MLflow è¿½è¸ªæ˜¯å¦å¯ç”¨ (`MLFLOW_ENABLED=true`)

**Q: å¯¼å‡ºæ•°æ®ä¸ºç©ºï¼Ÿ**
- ç¡®è®¤æœ‰å¯¹åº” rating çš„åé¦ˆæ•°æ®
- æ£€æŸ¥æ—¥æœŸèŒƒå›´è¿‡æ»¤æ¡ä»¶
