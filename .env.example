OPENAI_API_KEY="fillthisin"
ANTHROPIC_API_KEY="fillthisin"
GOOGLE_API_KEY="fillthisin"
GROQ_API_KEY="fillthisin"
DEEPSEEK_API_KEY="fillthisin"
# DeepSeek API 地址 (可选，默认官方地址)
# DEEPSEEK_API_BASE="https://api.deepseek.com/v1"

OPENAI_API_BASE=https://api.gptgod.online/v1

# ============================================================
# DSPy 基础配置
# ============================================================

# DSPy 缓存配置 (true/false)
# 设为 false 可禁用 LLM 调用缓存，强制每次重新请求
DSPY_CACHE_ENABLED="true"

# ============================================================
# 训练/编译参数配置
# ============================================================

# 并发线程数: 控制评估和优化时的并行度
# 增大可加速训练，但需注意 LLM API 的并发限制 (建议 1-8)
DSPY_NUM_THREADS="1"

# ============================================================
# MIPROv2 优化器参数
# ============================================================

# 候选提示词数量: 每轮生成多少个候选提示词进行评估
# 越大搜索空间越广，但耗时越长 (默认 10)
MIPRO_NUM_CANDIDATES="10"

# 初始温度: 控制提示词生成的随机性 (0.0-2.0)
# 较高温度产生更多样的候选，较低温度更保守 (默认 1.0)
MIPRO_INIT_TEMPERATURE="1.0"

# 批次数量: 优化迭代的总批次数
# 越多迭代越充分，但耗时越长 (默认 30)
MIPRO_NUM_BATCHES="30"

# 最大自举示例数: 从训练集自动生成的 few-shot 示例数量上限 (默认 8)
MIPRO_MAX_BOOTSTRAPPED_DEMOS="8"

# 最大标注示例数: 使用的人工标注示例数量上限 (默认 16)
MIPRO_MAX_LABELED_DEMOS="16"

# ============================================================
# BootstrapFewShot 优化器参数
# ============================================================

# 最大自举示例数: 自动生成的 few-shot 示例数量上限
# 太多会增加 token 消耗且可能引入噪声 (默认 4，建议 3-5)
BOOTSTRAP_MAX_BOOTSTRAPPED_DEMOS="4"

# 最大标注示例数: 使用的人工标注示例数量上限 (默认 4)
BOOTSTRAP_MAX_LABELED_DEMOS="4"

# ============================================================
# MLflow 集成配置
# ============================================================

# MLflow 启用开关 (true/false)
# 设为 false 可完全禁用 MLflow 追踪功能
MLFLOW_ENABLED="true"

# MLflow 追踪服务器地址
# 本地服务器: "http://localhost:5000" (默认，需要先启动 MLflow 服务器)
# 远程服务器: "https://your-mlflow-server.com"
MLFLOW_TRACKING_URI="http://localhost:5001"

# MLflow 实验名称
# 用于组织和分组相关的编译运行记录
MLFLOW_EXPERIMENT_NAME="dspyui-experiments"

# MLflow 本地存储配置 (仅在使用 bash webui.sh 启动本地服务器时生效)
# 后端数据库连接字符串 (默认: sqlite:///data/mlflow.db)
# MLFLOW_BACKEND_STORE_URI="sqlite:///data/mlflow.db"
# 工件存储根目录 (默认: ./mlartifacts)
# MLFLOW_ARTIFACT_ROOT="./mlartifacts"
# MLflow 服务器日志文件路径 (默认: data/mlflow.log)
# MLFLOW_LOG_FILE="data/mlflow.log"

# MLflow DSPy Autolog 配置
# 是否记录 LLM 调用追踪 (包含 tokens、延迟、成本等)
MLFLOW_LOG_TRACES="true"

# 是否记录编译过程中的 LLM 调用追踪
MLFLOW_LOG_TRACES_FROM_COMPILE="false"

# 是否记录评估过程中的 LLM 调用追踪
MLFLOW_LOG_TRACES_FROM_EVAL="true"

# 是否自动记录编译过程信息
MLFLOW_LOG_COMPILES="true"

# 是否自动记录评估过程信息
MLFLOW_LOG_EVALS="true"


# ============================================================
# API 服务配置
# ============================================================

# API 服务监听地址 (默认: 0.0.0.0)
API_HOST="0.0.0.0"

# API 服务监听端口 (默认: 8000)
API_PORT="8000"

# Uvicorn worker 数量 (默认: 4)
API_WORKERS="4"

# 请求超时时间（秒）(默认: 60)
API_REQUEST_TIMEOUT="60"

# ============================================================
# 模型加载配置
# ============================================================

# 默认模型阶段 (None/Staging/Production/Archived，默认: Production)
DEFAULT_MODEL_STAGE="Production"

# 是否启用模型缓存 (true/false，默认: true)
MODEL_CACHE_ENABLED="true"

# 模型缓存 TTL（秒）(默认: 3600)
MODEL_CACHE_TTL="3600"

# ============================================================
# 异步推理配置
# ============================================================

# 默认 LLM 模型 (默认: openai/gpt-4o-mini)
DEFAULT_LM="openai/gpt-4o-mini"

# 异步 LLM 调用 worker 数量 (默认: 4)
ASYNC_WORKERS="4"

# ============================================================
# 反馈配置
# ============================================================

# 是否启用反馈收集 (true/false，默认: true)
# 设为 false 时仍返回 trace_id，但不记录反馈
FEEDBACK_ENABLED="true"
